{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5aQJOQoryygIH/WoP9Fqk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKVWjU_5zJsN","executionInfo":{"status":"ok","timestamp":1732160517348,"user_tz":-330,"elapsed":12219,"user":{"displayName":"Mangalavinayagam Umashankar","userId":"10664991290733820132"}},"outputId":"413527ea-04fe-469b-c791-48a83411e8ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original model: SimpleModel(\n","  (fc1): Linear(in_features=128, out_features=64, bias=True)\n","  (relu): ReLU()\n","  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",")\n","Quantized model: SimpleModel(\n","  (fc1): DynamicQuantizedLinear(in_features=128, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n","  (relu): ReLU()\n","  (fc2): DynamicQuantizedLinear(in_features=64, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",")\n"]}],"source":["#Dynamic Quantization\n","\n","import torch\n","import torch.nn as nn\n","import torch.quantization\n","\n","class SimpleModel(nn.Module):\n","    def __init__(self):\n","        super(SimpleModel, self).__init__()\n","        self.fc1 = nn.Linear(128, 64)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = SimpleModel()\n","model.eval()\n","quantized_model = torch.quantization.quantize_dynamic(\n","    model, {nn.Linear}, dtype=torch.qint8\n",")\n","\n","print(\"Original model:\", model)\n","print(\"Quantized model:\", quantized_model)\n"]},{"cell_type":"code","source":["#Static Quantization\n","\n","import torch.quantization as quant\n","\n","model = SimpleModel()\n","model.eval()\n","\n","model.qconfig = quant.get_default_qconfig('fbgemm')\n","prepared_model = quant.prepare(model)\n","\n","def calibration_data():\n","    for _ in range(100):\n","        yield torch.randn(1, 128)\n","\n","for data in calibration_data():\n","    prepared_model(data)\n","\n","quantized_model = quant.convert(prepared_model)\n","\n","print(\"Quantized model:\", quantized_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XvTI70UezseS","executionInfo":{"status":"ok","timestamp":1732160535606,"user_tz":-330,"elapsed":414,"user":{"displayName":"Mangalavinayagam Umashankar","userId":"10664991290733820132"}},"outputId":"a3bd4790-d218-48de-bdda-6b0700c82960"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Quantized model: SimpleModel(\n","  (fc1): QuantizedLinear(in_features=128, out_features=64, scale=0.03221610561013222, zero_point=66, qscheme=torch.per_channel_affine)\n","  (relu): ReLU()\n","  (fc2): QuantizedLinear(in_features=64, out_features=10, scale=0.015236616134643555, zero_point=79, qscheme=torch.per_channel_affine)\n",")\n"]}]},{"cell_type":"code","source":["#QAT (Quantization Aware Training)\n","\n","class QATModel(nn.Module):\n","    def __init__(self):\n","        super(QATModel, self).__init__()\n","        self.fc1 = nn.Linear(128, 64)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","model = QATModel()\n","model.train()\n","\n","model.qconfig = quant.get_default_qat_qconfig('fbgemm')\n","qat_model = quant.prepare_qat(model)\n","\n","optimizer = torch.optim.SGD(qat_model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(5):\n","    optimizer.zero_grad()\n","    input_data = torch.randn(16, 128)\n","    output = qat_model(input_data)\n","    loss = nn.CrossEntropyLoss()(output, torch.randint(0, 10, (16,)))\n","    loss.backward()\n","    optimizer.step()\n","\n","qat_model.eval()\n","quantized_model = quant.convert(qat_model)\n","\n","print(\"Quantized model after QAT:\", quantized_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uLc1rDoMzzox","executionInfo":{"status":"ok","timestamp":1732160553285,"user_tz":-330,"elapsed":5467,"user":{"displayName":"Mangalavinayagam Umashankar","userId":"10664991290733820132"}},"outputId":"37bdb69a-c91b-48b1-814d-f279a41873b7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Quantized model after QAT: QATModel(\n","  (fc1): QuantizedLinear(in_features=128, out_features=64, scale=0.02763197384774685, zero_point=60, qscheme=torch.per_channel_affine)\n","  (relu): ReLU()\n","  (fc2): QuantizedLinear(in_features=64, out_features=10, scale=0.009817427955567837, zero_point=48, qscheme=torch.per_channel_affine)\n",")\n"]}]}]}